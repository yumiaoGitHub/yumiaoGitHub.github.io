---
layout:     post
title:      Long-term Recurrent Convolutional Networks for Visual Recognition and Description
subtitle:   人体动作识别论文阅读
date:       2017-11-23
author:     Mily
header-img: img/post-bg-cook.jpg
catalog: true
tags:

    - action recognition
---

### 摘要

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/6FDA64491B3E45B7A4E0B93F4663AA39/5208)

![img](yumiaoGithub.github.io\img\clipboard(168).png)

- **Many-to-one：序列输入，静态输出（**activity recognition**）**

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/E7C41B5D88E741E687B6AA08D0758988/5213)

- **One-to-many : 静态输入，序列输出（**image captioning**）**

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/F81DB7DA121C4889864C604292B7E5F9/5220)

- **Many-to-many：序列输入，序列输出（**video description**）**

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/1A9FAEB4CBD84A47B45782639ED6BC66/5226)



## 回顾RNN

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/6F52A573064948F08943879E7F19357C/5252)

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/6070D9B7BA4A469DB8A7853076B80A6C/5267)

LSTM 要理解的核心是 GATE，它就是靠这些 gate 的结构让信息有选择性的影响 RNN 中每个时刻的状态。所谓 gate 的结构就是一个使用 sigmoid 神经网络和一个 elementwise multiplication 的操作，这两个操作合在一起就是一个门的结构，sigmoid 作为激活函数的全连接神经网络层会输出一个 0-1 之间的数值，描述当前输入有多少信息量可以通过这个结构，类似于门，门打开时(sigmoid 输出为1时)，全部信息都可以通过；当门关上时(sigmoid 输出为0)，任何信息都无法通过。



![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/ADC99E57668340CBAA8B1ED2F0FC0315/5261)



![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/80F70D8CA95B4B98A0739A68C89CB77B/5263)

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/9E5EAF3D39E143159FABEC3ABEB25D1C/5269)

**Cell：信息舍弃（forget）--- 局部生成（input）--- 更新（+）---运算输出（output）**

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/0AF55B3AB8C842DE81B463946C53CBB0/5250)

《Show and Tell: A Neural Image Caption Generator 》

本文提出了一种encoder-decoder框架，其中通过CNN提取图像特征，然后经过LSTM生成目标语言，其目标函数为最大化目标描述的最大似然估计。 

![这里写图片描述](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/BF002E39A1A64479A5639E8311AAD245/5273)



模型主要包括encoder-decoder：encoder部分为一个用于提取图像特征的卷积神经网络，VGG16为例，输出最底层卷积14*14*512**conv5_feats特征向量**

a={a1...ai...aL} ,  作为LSTM输入。

decoder为经典的LSTM递归神经网络，其中第一步的输入为经过卷积神经网络提取的图像特征，其后时刻输入为每个单词的词向量表达。对于每个单词首先通过**one-hot向量[0,0,...,1,0,0...]**进行表示，然后经过**词嵌入模型（）**组合为输入。其中词嵌入矩阵W参与训练，达到相近语义单词距离较近的效果。

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/9366D224DC3947C9A83A09678B109F6B/5277)



## **本文应用-图片标注**

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/EA42044CB3064DF398E288CCE7B786BD/5355)

- 将CNN特征参数V作为每一时刻LSTM的输入，而不只是初始时刻的LSTM
- LSTM使用两层堆叠，事实验证效果并不好，并不是所有的结果都是越深越好！

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/176690A4474A420383D4997C64FBC8A0/5340)



![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/675C908DBC5F4E96B6C5DD63F3A07266/5424)

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/1171DA6D0E5D4584B6F39DE8096654D1/5420)

## **本文应用-动作识别**

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/3DC1B2FF27BD4E23A316D3E10FF781AF/5344)

**1.视频切片提取RGB帧（30fps）**

```
ffmpeg -i $1 -r $FRAMES $BNAME/$BNAME.%4d.jpg
```

（**UCF-101**每个视频片段长5s，每8帧取一个，共取16帧图片）

**2.根据RGB图片帧计算流帧** Compute flow frames:

**3. 训练单帧模型**：根据每帧图片通过CNN的结果直接预测

**4. 训练LRCN模型**

CNN的输出特征向量V可以通过batch_size批量同时生成，这里的LSTM是按照时间维度展开的，在时间序列上产生相应结果y1,y2,...,yt最后通过平均值计算再生成概率分布，得到最终分类。这相比于单帧模型参考整个视频的信息。



**LRCN flow VS LRCN RGB**

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/BE3F0C077302412DA3B71361416C6D47/5052)

结合视频动作识别的特殊性，有和特殊物品相关的行为（打字、刷牙）object识别至关重要，适合用RGB模型；有和动作幅度姿态相关的（拳击，篮球），此时flow模型更合适。

区别于视频动作识别每一帧图片通过CNN的特征参数Vi单独作为LSTM的输入（V1,V2,....,V16），视频描述是将从整个视频中采样的16帧图片的特征叠加在一起作为LSTM的输入（V=V1+V2+...+V16）因为后续视频的内容可能影响整体描述。

### 参考

- [递归神经网络RNN笔记](http://www.shuang0420.com/2017/07/21/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/)

