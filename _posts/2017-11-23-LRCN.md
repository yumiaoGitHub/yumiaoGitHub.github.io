---
layout:     post
title:      Long-term Recurrent Convolutional Networks for Visual Recognition and Description
subtitle:   人体动作识别论文阅读
date:       2017-11-23
author:     Mily
header-img: img/post-bg-cook.jpg
catalog: true
tags:

    - action recognition
---

### 摘要

![img](/../img/clipboard(168).png)

- **Many-to-one：序列输入，静态输出（**activity recognition**）**

  ![clipboard(174)](/../img/clipboard(174).png)

- **One-to-many : 静态输入，序列输出（**image captioning**）**

  ![clipboard(176)](/../img/clipboard(176).png)

- **Many-to-many：序列输入，序列输出（**video description**）**

  ![clipboard(164)](/../img/clipboard(164).png)

## 回顾RNN

![clipboard(166)](/../img/clipboard(166).png)![one_to_one](/../img/one_to_one.png)

LSTM 要理解的核心是 GATE，它就是靠这些 gate 的结构让信息有选择性的影响 RNN 中每个时刻的状态。所谓 gate 的结构就是一个使用 sigmoid 神经网络和一个 elementwise multiplication 的操作，这两个操作合在一起就是一个门的结构，sigmoid 作为激活函数的全连接神经网络层会输出一个 0-1 之间的数值，描述当前输入有多少信息量可以通过这个结构，类似于门，门打开时(sigmoid 输出为1时)，全部信息都可以通过；当门关上时(sigmoid 输出为0)，任何信息都无法通过。

![clipboard(172)](/../img/clipboard(172).png)



![clipboard(169)](/../img/clipboard(169).png)

![clipboard(171)](/../img/clipboard(171).png)

**Cell：信息舍弃（forget）--- 局部生成（input）--- 更新（+）---运算输出（output）**

**![clipboard(161)](/../img/clipboard(161).png)

《Show and Tell: A Neural Image Caption Generator 》

本文提出了一种encoder-decoder框架，其中通过CNN提取图像特征，然后经过LSTM生成目标语言，其目标函数为最大化目标描述的最大似然估计。

![122175645060](/../img/122175645060.png) 

模型主要包括encoder-decoder：encoder部分为一个用于提取图像特征的卷积神经网络，VGG16为例，输出最底层卷积14*14*512**conv5_feats特征向量**

a={a1...ai...aL} ,  作为LSTM输入。

decoder为经典的LSTM递归神经网络，其中第一步的输入为经过卷积神经网络提取的图像特征，其后时刻输入为每个单词的词向量表达。对于每个单词首先通过**one-hot向量[0,0,...,1,0,0...]**进行表示，然后经过**词嵌入模型（）**组合为输入。其中词嵌入矩阵W参与训练，达到相近语义单词距离较近的效果。

![img](https://note.youdao.com/yws/public/resource/d6eb4f6cd8b725d19739a5c95aa32365/xmlnote/9366D224DC3947C9A83A09678B109F6B/5277)



## **本文应用-图片标注**

![clipboard(175)](/../img/clipboard(175).png)

- 将CNN特征参数V作为每一时刻LSTM的输入，而不只是初始时刻的LSTM
- LSTM使用两层堆叠，事实验证效果并不好，并不是所有的结果都是越深越好！

![clipboard(163)](/../img/clipboard(163).png)



![clipboard(167)](/../img/clipboard(167).png)![clipboard(162)](/../img/clipboard(162).png)

## **本文应用-动作识别**

![clipboard(165)](/../img/clipboard(165).png)

**1.视频切片提取RGB帧（30fps）**

```
ffmpeg -i $1 -r $FRAMES $BNAME/$BNAME.%4d.jpg
```

（**UCF-101**每个视频片段长5s，每8帧取一个，共取16帧图片）

**2.根据RGB图片帧计算流帧** Compute flow frames:

**3. 训练单帧模型**：根据每帧图片通过CNN的结果直接预测

**4. 训练LRCN模型**

CNN的输出特征向量V可以通过batch_size批量同时生成，这里的LSTM是按照时间维度展开的，在时间序列上产生相应结果y1,y2,...,yt最后通过平均值计算再生成概率分布，得到最终分类。这相比于单帧模型参考整个视频的信息。



**LRCN flow VS LRCN RGB**

![clipboard(173)](/../img/clipboard(173).png)

结合视频动作识别的特殊性，有和特殊物品相关的行为（打字、刷牙）object识别至关重要，适合用RGB模型；有和动作幅度姿态相关的（拳击，篮球），此时flow模型更合适。

区别于视频动作识别每一帧图片通过CNN的特征参数Vi单独作为LSTM的输入（V1,V2,....,V16），视频描述是将从整个视频中采样的16帧图片的特征叠加在一起作为LSTM的输入（V=V1+V2+...+V16）因为后续视频的内容可能影响整体描述。

### 参考

- [递归神经网络RNN笔记](http://www.shuang0420.com/2017/07/21/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/)

