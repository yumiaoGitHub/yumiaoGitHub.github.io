---
layout:     post
title:      Long-term Recurrent Convolutional Networks for Visual Recognition and Description
subtitle:   人体动作识别论文阅读
date:       2017-11-23
author:     Mily
header-img: img/post-bg-cook.jpg
catalog: true
tags:

    - action recognition
---

### 摘要

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/2f441a12f072409e908ebc03831ab857/clipboard.png)

- **Many-to-one：序列输入，静态输出（**activity recognition**）**

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/4ab34c79972e4fa9825ec820c133287d/clipboard.png)

- **One-to-many : 静态输入，序列输出（**image captioning**）**

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/43df0129baf14f609a6145a93509b18e/clipboard.png)

- **Many-to-many：序列输入，序列输出（**video description**）**

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/36a457c1907a4a1fb77166b78721d57d/clipboard.png)



## 回顾RNN

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/a528e021403344ebb55f9e0a2efdc9a2/one_to_one.png)

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/42fbc9da7e7a4fa2877a7ad2f993e3a4/clipboard.png)

LSTM 要理解的核心是 GATE，它就是靠这些 gate 的结构让信息有选择性的影响 RNN 中每个时刻的状态。所谓 gate 的结构就是一个使用 sigmoid 神经网络和一个 elementwise multiplication 的操作，这两个操作合在一起就是一个门的结构，sigmoid 作为激活函数的全连接神经网络层会输出一个 0-1 之间的数值，描述当前输入有多少信息量可以通过这个结构，类似于门，门打开时(sigmoid 输出为1时)，全部信息都可以通过；当门关上时(sigmoid 输出为0)，任何信息都无法通过。

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/4e171d827d734522bb2916bc5b7b6fd0/clipboard.png)



![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/7fbefb2ea64947c79340c8a9941229b9/clipboard.png)

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/d99265950310409588ce6a7c64c542fd/clipboard.png)

**Cell：信息舍弃（forget）--- 局部生成（input）--- 更新（+）---运算输出（output）**

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/5cd65a2c25ad4bc1b383ac11d1044e66/clipboard.png)

《Show and Tell: A Neural Image Caption Generator 》

本文提出了一种encoder-decoder框架，其中通过CNN提取图像特征，然后经过LSTM生成目标语言，其目标函数为最大化目标描述的最大似然估计。 

![这里写图片描述](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/23852b8e04714b5fae42c04a8dea28f1/122175645060.png)



模型主要包括encoder-decoder：encoder部分为一个用于提取图像特征的卷积神经网络，VGG16为例，输出最底层卷积14*14*512**conv5_feats特征向量**

a={a1...ai...aL} ,  作为LSTM输入。

decoder为经典的LSTM递归神经网络，其中第一步的输入为经过卷积神经网络提取的图像特征，其后时刻输入为每个单词的词向量表达。对于每个单词首先通过**one-hot向量[0,0,...,1,0,0...]**进行表示，然后经过**词嵌入模型（）**组合为输入。其中词嵌入矩阵W参与训练，达到相近语义单词距离较近的效果。

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/59ba280055d149558951acc230bdf45b/clipboard.png)



## **本文应用-图片标注**

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/eb3a4750395045e5bc34ea34bbc49708/clipboard.png)

- 将CNN特征参数V作为每一时刻LSTM的输入，而不只是初始时刻的LSTM
- LSTM使用两层堆叠，事实验证效果并不好，并不是所有的结果都是越深越好！

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/c8c0925f38a64e67ac2f3c05361b81c3/clipboard.png)



![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/eedf85a4acf7446099d38ffb7a1adb56/clipboard.png)

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/16e3c75d89ba438fb8a707071774c261/clipboard.png)

## **本文应用-动作识别**

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/e6371f08503d453eaa844efd592a0325/clipboard.png)

**1.视频切片提取RGB帧（30fps）**

```
ffmpeg -i $1 -r $FRAMES $BNAME/$BNAME.%4d.jpg
```

（**UCF-101**每个视频片段长5s，每8帧取一个，共取16帧图片）

**2.根据RGB图片帧计算流帧** Compute flow frames:

**3. 训练单帧模型**：根据每帧图片通过CNN的结果直接预测

**4. 训练LRCN模型**

CNN的输出特征向量V可以通过batch_size批量同时生成，这里的LSTM是按照时间维度展开的，在时间序列上产生相应结果y1,y2,...,yt最后通过平均值计算再生成概率分布，得到最终分类。这相比于单帧模型参考整个视频的信息。



**LRCN flow VS LRCN RGB**

![img](D:/software/CloudDairy/qq17C623FD54406E2099CB9FC8FA19ADD4/b3f3017d42654e44ba38f9f90880481b/clipboard.png)

结合视频动作识别的特殊性，有和特殊物品相关的行为（打字、刷牙）object识别至关重要，适合用RGB模型；有和动作幅度姿态相关的（拳击，篮球），此时flow模型更合适。

区别于视频动作识别每一帧图片通过CNN的特征参数Vi单独作为LSTM的输入（V1,V2,....,V16），视频描述是将从整个视频中采样的16帧图片的特征叠加在一起作为LSTM的输入（V=V1+V2+...+V16）因为后续视频的内容可能影响整体描述。



[http://www.shuang0420.com/2017/07/21/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/](http://www.shuang0420.com/2017/07/21/递归神经网络 RNN 笔记/)